Okay, so now we're going to want some convenience codes to gather up all of the
parameters of the neural net so that we can operate on all of them
simultaneously. And every one of them, we will nudge a tiny amount based on the
gradient information. So let's collect the parameters of the neural net all in
one array.

So let's create a parameters of self that just returns self.w, which is a list,
concatenated with a list of self.b. So this will just return a list. List plus
list just gives you a list. So that's parameters of neuron. And I'm calling it
this way because also PyTorch has parameters on every single NN module, and it
does exactly what we're doing here. It just returns the parameter tensors. For
us, it's the parameter scalars.

Now layer is also a module, so it will have parameters. Cell and basically what
we want to do here is something like this: params is here, and then for neuron
in soft.neurons, we want to get neuron.parameters. We want to params.extend,
right? So these are the parameters of this neuron, and then we want to put them
on top of params, so params.extend of piece, and then we want to return params.

So there's way too much code, so actually, there's a way to simplify this,
which is return p for neuron in self.neurons for p in neuron.parameters. So
it's a single list comprehension. In Python, you can sort of nest them like
this, and you can then create the desired array.

So these are identical. We can take this out. And then let's do the same here:
def parameters self and return a parameter for layer in self.layers for p in
layer.parameters. And that should be good.

Now let me pop out this so we don't re-initialize our network because we need
to re-initialize our... Okay, so unfortunately, we will have to probably
re-initialize the network because we just had functionality because this class,
of course, I want to get all the end up parameters, but that's not going to
work because this is the old class.

Okay, so unfortunately we do have to reinitialize the network, which will
change some of the numbers, but let me do that so that we pick up the new API.
We can now do end up parameters, and these are all the weights and biases
inside the entire neural net. So in total, this MLP has 41 parameters.
