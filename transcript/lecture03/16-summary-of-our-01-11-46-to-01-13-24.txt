Okay, so I changed the code slightly. So we have here 200,000 steps of the
optimization. And in the first 100,000, we're using a learning rate of 0.1. And
then in the next 100,000, we're using a learning rate of 0.01. This is the loss
that I achieve. And these are the performance on the training and validation
loss. And in particular, the best validation loss I've been able to obtain in
the last 30 minutes or so is 2.17. So now I invite you to beat this number. And
you have quite a few knobs available to you to I think surpass this number. So
number one, you can of course change the number of neurons in the hidden layer
of this model. You can change the dimensionality of the embedding lookup table.
You can change the number of characters that are feeding in as an input as the
context into this model. And then of course you can change the details of the
optimization. How long are we running? What is the learning rate? How does it
change over time? How does it decay? You can change the the batch size and you
may be able to actually achieve a much better convergence speed in terms of how
many seconds or minutes it takes to train the model and get your result in
terms of really good loss. And then of course I actually invite you to read
this paper. It is 19 pages but at this point you should actually be able to
read a good chunk of this paper and understand pretty good chunks of it. And
this paper also has quite a few ideas for improvements that you can play with.
So all of those are knobs available to you, and you should be able to beat this
number. I'm leaving that as an exercise to the reader. And that's it for now,
and I'll see you next time.