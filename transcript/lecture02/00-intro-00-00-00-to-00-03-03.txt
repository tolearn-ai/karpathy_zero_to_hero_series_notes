Hi everyone, hope you're well. And next up what I'd like to do is I'd like to
build out MakeMore. Like microGrad before it, MakeMore is a repository that I
have on my GitHub webpage. You can look at it. But just like with microGrad,
I'm going to build it out step by step, and I'm going to spell everything out.
So we're going to build it out slowly and together. Now, what is MakeMore?
MakeMore, as the name suggests, makes more of things that you give it. So
here's an example. Names.txt is an example data set to make more. And when you
look at names.txt, you'll find that it's a very large data set of names. So
here's lots of different types of names. In fact, I believe there are 32,000
names that I sort of found randomly on a government website. And if you train
make more on this data set, it will learn to make more of things like this. And
in particular, in this case, that will mean more things that sound name-like
but are actually unique names. And maybe if you have a baby and you're trying
to assign a name, maybe you're looking for a cool new sounding unique name,
make more might help you. So here are some example generations from the neural
network once we train it on our data set. So here's some example unique names
that it will generate. Dontel, Irot, Zendy, and so on. And so all these sort of
sound name-like, but they're not, of course, names. So under the hood, MakeMore
is a character-level language model. So what that means is that it is treating
every single line here as an example. And within each example, it's treating
them all as sequences of individual characters. So R-E-E-S-E is this example,
and that's the sequence of characters, and that's the level on which we are
building out MakeMore. And what it means to be a character-level language model
then is that it's just sort of modeling those sequences of characters, and it
knows how to predict the next character in the sequence. Now, we're actually
going to implement a large number of character-level language models in terms
of the neural networks that are involved in predicting the next character in a
sequence. So very simple bigram and bag-of-word models, multilayered
perceptrons, recurrent neural networks, all the way to modern transformers. In
fact the transformer that we will build will be basically the equivalent
transformer to GPT-2 if you have heard of GPT. So that's kind of a big deal,
it's a modern network and by the end of the series you will actually understand
how that works on the level of characters. Now to give you a sense of the
extensions here, after characters we will probably spend some time on the word
level so that we can generate documents of words not just little you know
segments of characters but we can generate entire large much larger documents
and then we're probably going to go into images and image text networks such as
DALI, stable diffusion and so on but for now we have to start here character
level language modeling let's go