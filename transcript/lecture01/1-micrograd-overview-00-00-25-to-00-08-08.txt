take you through building of micrograd now micrograd is this library that i
released on github about two years ago but at the time i only uploaded the
source code and you'd have to go in by yourself and really figure out how it
works so in this lecture i will take you through it step by step and kind of
comment on all the pieces of it so what's micrograd and why is it interesting
okay micrograd is basically an autograd engine autograd is short for automatic
gradient and And really what it does is it implements backpropagation. Now,
backpropagation is this algorithm that allows you to efficiently evaluate the
gradient of some kind of a loss function with respect to the weights of a
neural network. And what that allows us to do then is we can iteratively tune
the weights of that neural network to minimize the loss function and therefore
improve the accuracy of the network. So backpropagation would be at the
mathematical core of any modern deep neural network library, like say PyTorch
or JAX. So the functionality of micrograd is I think best illustrated by an
example. So if we just scroll down here you'll see that micrograd basically
allows you to build out mathematical expressions and here what we are doing is
we have an expression that we're building out where you have two inputs a and b
and you'll see that a and b are negative four and two but we are wrapping those
values into this value object that we are going to build out as part of
micrograd. So this value object will wrap the numbers themselves. And then we
are going to build out a mathematical expression here where A and B are
transformed into C, D, and eventually E, F, and G. And I'm showing some of the
functionality of microgrid and the operations that it supports. So you can add
two value objects. You can multiply them. You can raise them to a constant
power. You can offset by one, negate, squash at zero, square, divide by
constant, divide by it, et cetera. And so we're building out an expression
graph with these two inputs a and b, and we're creating an output value of g.
And micrograd will, in the background, build out this entire mathematical
expression. So it will, for example, know that c is also a value, c was a
result of an addition operation, and the child nodes of c are a and b, and it
will maintain pointers to a and b value objects. so we'll basically know
exactly how all of this is laid out and then not only can we do what we call
the forward pass where we actually look at the value of g of course that's
pretty straightforward we will access that using the dot data attribute and so
the output of the forward pass the value of g is 24.7 it turns out but the big
deal is that we can also take this g value object and we can call dot backward
and this will basically initialize back propagation at the node g and what
backpropagation is going to do is it's going to start at g and it's going to go
backwards through that expression graph and it's going to recursively apply the
chain rule from calculus and what that allows us to do then is we're going to
evaluate basically the derivative of g with respect to all the internal nodes
like e d and c but also with respect to the inputs a and b and then we can
actually query this derivative of g with respect to a for example that's a dot
grad in this case it happens to be 138 and the derivative of g with respect to
b which also happens to be here 645 and this derivative we'll see soon is very
important information because it's telling us how a and b are affecting g
through this mathematical expression so in particular, a.grad is 138. So if we
slightly nudge a and make it slightly larger, 138 is telling us that g will
grow, and the slope of that growth is going to be 138. And the slope of growth
of b is going to be 645. So that's going to tell us about how g will respond if
a and b get tweaked a tiny amount in a positive direction. Okay. Now, you might
be confused about what this expression is that we built out here. And this
expression, by the way, is completely meaningless. I just made it up. I'm just
flexing about the kinds of operations that are supported by micrograd. What we
actually really care about are neural networks. But it turns out that neural
networks are just mathematical expressions, just like this one, but actually a
slightly bit less crazy even. Neural networks are just a mathematical
expression. They take the input data as an input, and they take the weights of
a neural network as an input. And it's a mathematical expression, and the
output are your predictions of your neural net or the loss function. We'll see
this in a bit. But basically neural networks just happen to be a certain class
of mathematical expressions. But backpropagation is actually significantly more
general. It doesn't actually care about neural networks at all. It only cares
about arbitrary mathematical expressions and then we happen to use that
machinery for training of neural networks. Now one more note I would like to
make at this stage is that as you see here micrograd is a scalar valued
autograd engine. so it's working on the you know level of individual scalars
like negative four and two and we're taking neural nets and we're breaking them
down all the way to these atoms of individual scalars and all the little pluses
and times and it's just excessive and so obviously you would never be doing any
of this in production it's really just done for pedagogical reasons because it
allows us to not have to deal with these n-dimensional tensors that you would
use in modern deep neural network library so this is really done so that you
understand and refactor out background application and chain rule and
understanding of neural training and then if you actually want to train bigger
networks you have to be using these tensors but none of the math changes this
is done purely for efficiency we are basically taking scale value all the scale
values we're packaging them up into tensors which are just arrays of these
scalars and then because we have these large arrays we're making operations on
those large arrays that allows us to take advantage of the parallelism in a
computer and all those operations can be done in parallel and then the whole
thing runs faster but really none of the math changes and they're done purely
for efficiency so i don't think that it's pedagogically useful to be dealing
with tensors from scratch and i think and that's why i fundamentally wrote
micrograd because you can understand how things work at the fundamental level
and then you can speed it up later okay so here's the fun part my claim is that
micrograd is what you need to train neural networks and everything else is just
efficiency. So you'd think that micrograd would be a very complex piece of code
and that turns out to not be the case. So if we just go to micrograd and you'll
see that there's only two files here in micrograd. This is the actual engine,
it doesn't know anything about neural nets and this is the entire neural nets
library on top of micrograd. So engine and nn.py. So the actual backpropagation
autograd engine that gives you the power of neural networks is literally 100
lines of code of like very simple python which we'll understand by the end of
this lecture and then nn.py this neural network library built on top of the
autograd engine is like a joke it's like we have to define what is a neuron and
then we have to define what is a layer of neurons and then we define what is a
multilateral perceptron, which is just a sequence of layers of neurons. And so
it's just a total joke. So basically, there's a lot of power that comes from
only 150 lines of code. And that's all you need to understand to understand
neural network training and everything else is just efficiency. And of course,
there's a lot to efficiency, but fundamentally, that's all that's happening.